\documentclass[12pt]{report}
\usepackage{fullpage}
\usepackage{mathptmx}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{color}
\author{Rober Boshra}
\title{Matrix Multiplication Using MPI\\ECE 709, Assignment 1}
\begin{document}


\definecolor{javared}{rgb}{0.6,0,0} % for strings
\definecolor{javagreen}{rgb}{0.25,0.5,0.35} % comments
\definecolor{javapurple}{rgb}{0.5,0,0.35} % keywords
\definecolor{javadocblue}{rgb}{0.25,0.35,0.75} % javadoc
 
\lstset{language=C,
frame=tb,
basicstyle=\ttfamily,
keywordstyle=\color{javapurple}\bfseries,
stringstyle=\color{javared},
commentstyle=\color{javagreen},
morecomment=[s][\color{javadocblue}]{/**}{*/},
numbers=left,
numberstyle=\tiny\color{black},
stepnumber=2,
numbersep=10pt,
tabsize=2,
showspaces=false,
showstringspaces=false,
title=\lstname}

\maketitle
\chapter{Introduction}
As part of the first assignment, I implemented parallel matrix multiplication utilizing the 3 main MPI communication methods: blocking point-to-point (P2P), non-blocking P2P, and collective. In the present report I will discuss each of my implementations in order discussing running time and factors affecting it.

The general definition of a matrix product is the following: for matrix A of size n $\times$ m and matrix B of size m $\times$ p, 
\begin{equation}
(AB)_{ij}=\sum\limits_{k=1}^m A_{ik}B_{kj}
\end{equation}
The main algorithm used for all three different implementation used the property of matrices such that the matrix multiplication of matrices A and B forming the matrix C can be calculated in parallel by multiplying individual rows of A by the matrix B such that for rows $x \subset i$,
\begin{equation}
C_{xj} = A_{xk} \times B_{kj}
\end{equation}
Given the independence of results for each subset x, rows can be divided across processors available for computation. In cases where the rows are not divisible by the number of processors, the last processor can process the extra remaining rows. Note that adding the last rows to the last single process's load might not be optimal. This will be discussed further in the last chapter. 
As per the assignment instructions, sizes of matrices A and B are defined as $N \times 32$ and $32 \times N$, respectively. This reduces equation 1.2 to:
\begin{equation}
C_{xj} = A_{xk} \times B_{ki}
\end{equation}
where k [0,31]\footnote{\label{^1}Zero indexing is assumed for consistency between the equations and the C code presented below.}.

\chapter{Communication-specific Implementations}
\section{Blocking P2P communication}
To accomplish a parallel implementation of matrix multiplication using blocking communication between processes, the equation highlighted in 1.3 was used. Each matrix was represented as a 2D array of floats and kept on the stack. No heap allocation (malloc/calloc) of memory was used and that was seen as sufficient for development purposes.
As a user of an i5 processor with 4 cores, I am able to run a total of 4 processes (the upper limit as provided in the assignment instructions. 

\section{Non-blocking P2P communication}

\section{Collective communication}

\chapter{General Discussion}

\appendix
\chapter*{Code appendix}

\lstinputlisting[language=bash, breaklines]{makefile}
\lstinputlisting[breaklines]{blocking.c}
\lstinputlisting[breaklines]{nonblocking.c}
\lstinputlisting[breaklines]{collective.c}




\end{document}